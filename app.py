# app.py

from loader.yuque_loader import YuqueLoader
from loader.text_preprocessor import TextPreprocessor
from embedder.bc_embedding import BCEmbeddingWrapper
from vectorstore.faiss_store import FaissVectorStore
from retriever.rerank_retriever import RerankRetriever
from llm.ollama_llm import OllamaLLM
from llm.openai_llm import OpenAILLM

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from config import (
    FAISS_INDEX_PATH,
    VECTOR_DIM,
    BC_EMBED_MODEL,
    BC_RERANK_MODEL,
    OLLAMA_MODEL,
    USE_OPENAI,
    OPENAI_API_KEY,
    OPENAI_MODEL,
    OPENAI_API_BASE,
    QA_MODE,
    TOP_K_INITIAL,
    TOP_K_RERANK,
    YUQUE_TOKEN,
    YUQUE_GROUP,
    YUQUE_NAMESPACE
)

def initialize_retriever_and_llm():
    # ===== é…ç½®éƒ¨åˆ† =====
    yuque_token = YUQUE_TOKEN
    group_login = YUQUE_GROUP            # å›¢é˜Ÿåï¼ˆåŠ è½½æ•´ä¸ªå›¢é˜ŸçŸ¥è¯†åº“ï¼‰
    namespace = YUQUE_NAMESPACE          # ä¸ä¸ºç©ºåˆ™åŠ è½½å•ä¸€çŸ¥è¯†åº“

    embed_model_name = BC_EMBED_MODEL
    rerank_model_name = BC_RERANK_MODEL

    # ===== å°è¯•åŠ è½½å·²æœ‰ç´¢å¼• =====
    faiss_store = FaissVectorStore(vector_dim=VECTOR_DIM, index_path=FAISS_INDEX_PATH)
    index_exists = os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_INDEX_PATH + ".docs.pkl")

    # ===== åŠ è½½BCEmbeddingæ¨¡å‹ï¼ˆé¢„å¤„ç†æ¨¡å¼å’Œé—®ç­”æ¨¡å¼éƒ½ç”¨åˆ°ï¼‰=====
    print("ğŸ§  åˆå§‹åŒ– BCEmbedding æ¨¡å‹...")
    bc_embedding = BCEmbeddingWrapper(embed_model_name, rerank_model_name)

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç´¢å¼•åŠæ–‡æ¡£ï¼Œæˆ–å…³é—­äº†çº¯é—®ç­”æ¨¡å¼
    if not index_exists or not QA_MODE:
        print("ğŸ§© è¿›å…¥æ•°æ®æŒä¹…åŒ–æ¨¡å¼...")

        # ===== åŠ è½½æ–‡æ¡£ =====
        print("ğŸ” æ­£åœ¨åŠ è½½è¯­é›€çŸ¥è¯†åº“æ–‡æ¡£...")
        loader = YuqueLoader(yuque_token)
        documents = loader.load_documents(group_login=group_login, namespace=namespace)
        print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ç¯‡æ–‡æ¡£")

        # ===== æ–‡æœ¬é¢„å¤„ç†ï¼ˆæ¸…æ´—ä¸åˆ‡åˆ†ï¼‰ =====
        print("ğŸ§¹ æ­£åœ¨æ¸…æ´—ä¸åˆ‡åˆ†æ–‡æ¡£...")
        preprocessor = TextPreprocessor()
        documents = preprocessor.process_documents(documents)
        print(f"âœ… æ¸…æ´—ä¸åˆ‡åˆ†å®Œæˆï¼Œå…± {len(documents)} æ®µæ–‡æœ¬")


        # ===== å‘é‡åŒ–æ–‡æ¡£å†…å®¹ =====
        print("ğŸ”¢ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£å‘é‡...")
        texts = [doc.page_content for doc in documents]
        embeddings = bc_embedding.embed_texts(texts)

        # ===== æ„å»ºå‘é‡ç´¢å¼• =====
        print("ğŸ“¦ æ„å»º FAISS å‘é‡ç´¢å¼•...")
        faiss_store.add_embeddings(embeddings, documents)
        faiss_store.save()
        print("âœ… æ•°æ®æŒä¹…åŒ–å®Œæˆ")

    else:
        print("ğŸ“¥ åŠ è½½å·²æœ‰å‘é‡ç´¢å¼•...")
        faiss_store.load()

    documents = faiss_store.documents

    # ===== æ£€ç´¢å™¨ï¼ˆå¸¦é‡æ’åºï¼‰ =====
    retriever = RerankRetriever(
        faiss_store=faiss_store,
        bc_embedding_wrapper=bc_embedding,
        documents=documents,
        top_k_initial=TOP_K_INITIAL,
        top_k_rerank=TOP_K_RERANK
    )

    # ===== åˆå§‹åŒ– LLM =====
    if USE_OPENAI:
        print("ğŸŒ ä½¿ç”¨ OpenAI API æ¨¡å‹")
        llm = OpenAILLM(
            model_name=OPENAI_MODEL,
            api_key=OPENAI_API_KEY,
            api_base=OPENAI_API_BASE  # âœ… ä¼ å…¥ base_url
        )
    else:
        print("ğŸ–¥ï¸ ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹")
        llm = OllamaLLM(OLLAMA_MODEL)

    return retriever, llm

def run_cli_loop(retriever, llm):
    # ===== å‘½ä»¤è¡Œé—®ç­”å¾ªç¯ =====
    print("\nğŸ¤ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰")
    while True:
        query = input("\nğŸ§¾ ä½ çš„é—®é¢˜ï¼š").strip()
        if query.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿ")
            break

        # æ£€ç´¢æ–‡æ¡£ â†’ æ„é€ ä¸Šä¸‹æ–‡
        relevant_docs = retriever.invoke(query)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # æ„é€  Prompt
        prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{query}\n\nå›ç­”ï¼š"

        # è°ƒç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›ç­”
        answer = llm.generate(prompt)
        print(f"\nğŸ¤– å›ç­”ï¼š{answer}")

if __name__ == "__main__":
    retriever, llm = initialize_retriever_and_llm()
    run_cli_loop(retriever, llm)